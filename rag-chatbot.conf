GET /_cluster/allocation/explain

GET /_cluster/health

GET /_plugins/_ml/tasks/sF4Qc5QB7ZrY_Tnjmpp1

GET _nodes/stats/breaker

GET /_cat/plugins

GET /opensearch_logs/_mapping

GET /_cat/nodes

GET opensearch_logs/_settings

GET /_cat/indices

GET _dangling?pretty

GET /_cat/shards?v

POST /_plugins/_ml/models/sVvBoZQBhF1LOrE9IVod/_deploy

POST /_cluster/reroute?retry_failed=true

POST _plugins/_ml/models/oYOsl5QBSHwpkug3fKIT/_clear_cache

POST /_plugins/_ml/models/sVvBoZQBhF1LOrE9IVod/_undeploy

POST _cache/clear

DELETE /_cat/_ml/trained_models/oYOsl5QBSHwpkug3fKIT

DELETE /_plugins/_ml/models/oYOsl5QBSHwpkug3fKIT

DELETE /opensearch_logs

DELETE _ingest/pipeline/cluster_pipe

# данные о памяти разговора
GET /_plugins/_ml/memory/gQ75lI0BHcHmo_cz2acL

# возвращает все сообщения в разговоре
GET /_plugins/_ml/memory/gQ75lI0BHcHmo_cz2acL/messages

# данные о конкретном сообщении
GET /_plugins/_ml/memory/message/gg75lI0BHcHmo_cz2acZ

# trace data для сообщения
GET /_plugins/_ml/memory/message/gg75lI0BHcHmo_cz2acZ/traces

POST _cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "security-auditlog-2025.01.08",
        "shard": 0,
        "node": "data1",
        "accept_data_loss": true
      }
    }
  ]
}

GET /_cluster/settings/include_defaults=true&pretty

PUT /_cluster/settings
{
  "persistent": {
    "plugins.ml_commons.only_run_on_ml_node": true,
    "plugins.ml_commons.max_model_on_node": 15,
    "plugins.ml_commons.allow_registering_model_via_local_file": true,
    "plugins.ml_commons.allow_registering_model_via_url": true,
    "plugins.ml_commons.connector_access_control_enabled": true,
    "plugins.ml_commons.agent_framework_enabled": true,
    "plugins.ml_commons.memory_feature_enabled": true,
    "plugins.ml_commons.rag_pipeline_feature_enabled": true,
    "plugins.ml_commons.ml_task_timeout_in_seconds": 1800,
    "plugins.ml_commons.native_memory_threshold": 100,
    "plugins.ml_commons.model_auto_redeploy.enable": false,
    "plugins.ml_commons.local_model.enabled": true,
    "plugins.ml_commons.trusted_connector_endpoints_regex": [
            "^http://192\\.168\\.50\\.194:8880/ask$",
            "^http://192\\.168\\.50\\.194:11434/api/generate$",
            "^http://localhost:11434/api/generate$",
            "^http://127.0.0.1:11434/api/generate$",
            "^http://127\\.0\\.0\\.1:8880/ask$",
            "^http://host\\.docker\\.internal:8880/ask$"
        ],
    "plugins.ml_commons.connector.private_ip_enabled": true,
    "plugins.ml_commons.jvm_heap_memory_threshold": 100,
    "plugins.ml_commons.disk_free_space_threshold": -1,
    "cluster.fault_detection.follower_check.timeout": "30s",
    "cluster.fault_detection.leader_check.timeout": "30s"
  }
}

# группа для эмбеддинг и ЛЛМ моделей
POST /_plugins/_ml/model_groups/_register
{
    "name": "group for embedding models",
    "description": "My embedding models' group"
}

#  "model_group_id": "zkzAoZQBuN6ar9ilrJwW",

# регистрация мультиязычной эмбеддинг модели 'paraphrase-multilingual-MiniLM-L12-v2'
POST /_plugins/_ml/models/_register
{
  "name": "huggingface/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
  "version": "1.0.1",
  "model_group_id": "zkzAoZQBuN6ar9ilrJwW",
  "model_format": "ONNX"
}

#  "model_id": "kwht-pQBde_U-MtoHnRmd"

POST _plugins/_ml/controllers/kwht-pQBde_U-MtoHnRm
{
  "user_rate_limiter": {
    "admin": {
      "limit": 6,
      "unit": "MINUTES"
    }
  }
}

# deployment embedding модели
POST /_plugins/_ml/models/kwht-pQBde_U-MtoHnRm/_deploy

# тест эмбеддинг модели 
POST /_plugins/_ml/_predict/text_embedding/kwht-pQBde_U-MtoHnRm
{
  "text_docs":["Kazakhstan"],
  "return_number": true,
  "target_response": ["sentence_embedding"]
}

##################################################################
# ВЕКТОРИЗАЦИЯ ВХОДЯЩИХ ЛОГОВ

# Ingest pipeline
PUT /_ingest/pipeline/logs_pipe
{
  "description": "pipeline for incoming logs",
  "processors": [
    {
      "text_embedding": {
        "model_id": "kwht-pQBde_U-MtoHnRm",
        "field_map": {
          "tail.log": "log_embed"
        }
      }
    }
  ]
}

PUT /opensearch_logs
{
  "settings": {
    "index.knn": true,
    "default_pipeline": "logs_pipe"
  },
  "mappings": {
    "properties": {
      "tail.log": {
        "type": "text"
      },
      "tail.log_embed": {
        "type": "knn_vector",
        "dimension": 384,
        "method": {
          "engine": "lucene",
          "space_type": "l2",
          "name": "hnsw",
          "parameters": {}
        }
      }
    }
  }
}


# проверка работы эмбеддинг модели
# KNN поиск
GET /opensearch_logs/_search
{
  "_source": {
    "excludes": ["tail.log_embed", "tail.loglevel_embed", "tail.tail.log_embed", "tail.tail.loglevel_embed"]
  },
  "query": {
    "neural": {
      "tail.loglevel_embed": {
        "query_text": "error log documents",
        "model_id": "kwht-pQBde_U-MtoHnRm",
        "k": 100
      }
    }
  }
}

#########################################################################
# Коннектор к OLLAMA API (модель DeepSeek-r1)

POST /_plugins/_ml/connectors/_create
{
    "name": "DeepSeek-r1",
    "description": "The connector to Deepseek-r1 LLM model hosted on the local machine",
    "version": 1,
    "protocol": "http",
    "parameters": {
        "endpoint": "192.168.50.194:11434",
        "model": "DeepSeek-R1:latest",
        "stream": false,
        "response_filter": "$.response"
    },
    "credential": {},
    "actions": [
        {
            "action_type": "predict",
            "method": "POST",
            "url": "http://${parameters.endpoint}/api/generate",
            "request_body": "{ \"model\": \"${parameters.model}\", \"prompt\": \"${parameters.prompt}\", \"stream\": false }"
        }
    ]
}


#  "connector_id": "6SSb1ZQBX7j6uPTv4JPL"

# регистраиция deepseek коннектора
POST /_plugins/_ml/models/_register
{
    "name": "deepseek-r1",
    "function_name": "remote",
    "model_group_id": "zkzAoZQBuN6ar9ilrJwW",
    "description": "remote text generation model",
    "connector_id": "6SSb1ZQBX7j6uPTv4JPL"
}

#  "model_id": "7SSc1ZQBX7j6uPTvAJNz"

# проверка работы LLM DeepSeek
POST /_plugins/_ml/models/7SSc1ZQBX7j6uPTvAJNz/_predict
{
  "parameters": {
    "prompt": "Hi how are you doing?"
  }
}


POST _plugins/_ml/agents/_register
{
  "name": "Chat Agent with Deepseek-R1",
  "type": "conversational",
  "description": "You are AI RAG-chatbot agent aimed at searching semantically close documents in 'opensearch_logs' index and answering human questions. Always check out tools' descriptions so you could decide which tool to use for the best answer generation. In 95% of all human questions you use VectorDBTool to retrieve data from 'opensearch_logs' index",
  "app_type": "os_chat",
  "llm": {
    "model_id": "7SSc1ZQBX7j6uPTvAJNz",
    "parameters": {
      "max_iteration": 2,
      "response_filter": "$.response",
      "message_history_limit": 5,
      "disable_trace": false
    }
  },
  "memory": {
    "type": "conversation_index"
  },
  "tools": [
    {
      "type": "IndexMappingTool",
      "name": "IndexMappingTool",
      "parameters": {
        "index": [ "opensearch_logs" ],
        "input": "${parameters.question}"
      }
    },
    {
      "type": "SearchIndexTool",
      "index": "opensearch_logs",
      "description": "Use first IndexMappingTool to get the mapping of 'opensearch_logs' index so you could create a correct DSL query"
    },
    {
      "type": "VisualizationTool",
      "parameters": {
        "index": "opensearch_logs"
      },
      "include_output_in_agent_response": true
    },
    {
      "type": "VectorDBTool",
      "desccription": "Use this tool to search log statuses in 'opensearch_logs' index and the log descriptions",
      "parameters": {
        "model_id": "kwht-pQBde_U-MtoHnRm",
        "index": "opensearch_logs",
        "embedding_field": "tail.log_embed",
        "source_field": ["tail.log"],
        "input": "${parameters.question}"
      }    
    },
    {
      "type": "CatIndexTool",
      "description": "Use this tool to get OpenSearch index information: (health, status, index, uuid, primary count, replica count, docs.count, docs.deleted, store.size, primary.store.size). \nIt takes 2 optional arguments named `index` which is a comma-delimited list of one or more indices to get information from (default is an empty list meaning all indices), and `local` which means whether to return information from the local node only instead of the cluster manager node (default is false)."
    },
    {
      "type": "SearchAnomalyDetectorsTool",
      "description": "Use this tool only if user is asking to search for anomalies."
    },
    {
      "type": "SearchAnomalyResultsTool",
      "description": "Use this tool only if user is asking to search for anomalies."
    },
    {
      "type": "SearchMonitorsTool"
    },
    {
      "type": "SearchAlertsTool",
      "description": "Use this tool only if user is asking to search for alerts."
    }
  ]
}


  POST /_plugins/_ml/agents/Ti6n_pQBP9SKyVIJqWrm/_execute
{
  "parameters": {
    "question": "use VectorDBTool to find last ERROR logged documents in opensearch_logs and return their descriptions?",
    "verbose": true
  }
}

#  "agent_id": UiVO25QBX7j6uPTvAMyj

POST /_plugins/_ml/agents/Ti6n_pQBP9SKyVIJqWrm/_execute
{
  "parameters": {
    "question": "Now check Warn logged documents",
    "next_action": "Compare how influencial they are to the cluster work",
    "memory_id": "WRvX_ZQBmb9_BD8Yh8g3",
    "message_history_limit": 8
  }
}



PUT .plugins-ml-config/_doc/os_chat
{
    "type":"os_chat_root_agent",
    "configuration":{
        "agent_id": "dy5WBJUBP9SKyVIJ9Hlz"
    }
}





POST /_plugins/_ml/agents/_register
{
    "name": "OpenSearch and VM logs analysis RAG-chatbot agent",
    "type": "conversational_flow",
    "description": "My RAG-chatbot agent for logs analysis of Opensearch and virtual machine",
    "app_type": "rag",
    "memory": {
        "type": "conversation_index"
    },
    "tools": [
        {
            "type": "CatIndexTool"
        },
        {
            "type": "SearchAlertsTool"
        },
        {
            "type": "VectorDBTool",
            "name": "VectorDBLogTool",
            "parameters": {
                "model_id": "kwht-pQBde_U-MtoHnRm",
                "index": "opensearch_logs",
                "embedding_field": "tail.log_embed",
                "source_field": [
                    "tail.log", "tail.loglevel"
                ],
                "input": "${parameters.question}",
                "doc_size": 8
            }
        },
        {
            "type": "VectorDBTool",
            "name": "VectorDBLevelTool",
            "parameters": {
                "model_id": "kwht-pQBde_U-MtoHnRm",
                "index": "opensearch_logs",
                "embedding_field": "tail.loglevel_embed",
                "source_field": [
                    "tail.log"
                ],
                "input": "${parameters.question}",
                "doc_size": 8
            }
        },
        {
            "type": "MLModelTool",
            "name": "DeepSeek-R1-7B",
            "description": "My LLM for generation responses to human questions",
            "parameters": {
                "model_id": "7SSc1ZQBX7j6uPTvAJNz",
                "prompt": "Human:You are a professional data analysist. You will always answer question based on the given context first. If the answer is not directly shown in the context, you will analyze the data and find the answer.\nCONTEXT:\nSome extra information: Opensearch cluster indices: \n${parameters.catIndexTool.output:-};\n Detected alerts: ${parameters.SearchAlertsTool.output:-}\n Use above stated extra information if it's helpful to answer the human's question.\nThe following data was retrieved from vector data base and covers 95% of all human's questions: \n${parameters.VectorDBLogTool.output:-}\n${parameters.VectorDBLevelTool.output:-}\nHere is the chat history:\n${parameters.chat_history:-}\n\nHuman:${parameters.question}\nAssistant:"
            }
        }
    ]
}


POST /_plugins/_ml/agents/5C-dE5UBP9SKyVIJHpn7/_execute
{
  "parameters": {
    "question": "what processes initiated warn logs?"
  }
}


POST /_plugins/_ml/agents/5C-dE5UBP9SKyVIJHpn7/_execute
{
  "parameters": {
    "question": "Do you think these operations critical for the work?",
    "next_action": "Your advices",
    "memory_id": "LBtED5UBmb9_BD8YKck9",
    "message_history_limit": 5,
    "prompt": "Human:You are a professional data analysist. You will always answer question based on the given context first. If the answer is not directly shown in the context, you will analyze the data and find the answer.\nCONTEXT:\nSome extra information: Opensearch cluster indices: \n${parameters.catIndexTool.output:-};\n Detected alerts: ${parameters.SearchAlertsTool.output:-}\n Use above stated extra information if it's helpful to answer the human's question.\nThe following data was retrieved from vector data base and in cover 95% of all human's questions: \n${parameters.VectorDBLogTool.output:-}\n${parameters.VectorDBLevelTool.output:-}\nHere is the chat history:\n${parameters.chat_history:-}\n\nHuman:${parameters.question}\nAssistant:"
  }
}
